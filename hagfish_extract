#!/usr/bin/env python

import os
import sys
import pickle

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab

import logging
import optparse
import subprocess

## Arguments: General options 
parser = optparse.OptionParser()
parser.add_option('-v', dest='verbose', action="count", 
                  help='Show debug information')
parser.add_option('-x', dest='limit', type="int",
                  help='Limit the number of pairs to read per input file')
parser.add_option('--onlySeqInfo', dest='onlySeqInfo', action='store_true',
                  help='only process the seqInfo of the first bam input file')

parser.add_option('--skip_read' , dest='skipRead', action="store_true",
                  help='Skip reading the BAM files, read the parsed BAM files ' + 
                  'directly from disk')
parser.add_option('--skip_stats' , dest='skipStat', action="store_true",
                  help='Skip stat creating, read the stat data from disk')

parser.add_option('--skip_coverage' , dest='skipCoverage', action="store_true",
                  help='Skip generating of the coverage plots')

parser.set_defaults(maxInsert='20000')
parser.add_option('--max', dest='maxInsert', 
                   help='max insert size to consider while creating the coverage plots')

parser.set_defaults(minLen='10000')
parser.add_option('--minLen', dest='minLen',
                  help='Minimal sequence length - ignore sequence shorter than ' +
                  'this length')

options, args = parser.parse_args()
inputFiles = args
l = logging.getLogger('hagfish')
handler = logging.StreamHandler()
logmark = chr(27) + '[0;37;44mHAGFISH' + \
          chr(27) + '[0m ' 

formatter = logging.Formatter(
    logmark + '%(levelname)-6s %(message)s')

handler.setFormatter(formatter)
l.addHandler(handler)

if options.verbose >= 2:
    l.setLevel(logging.DEBUG)
elif options.verbose == 1:
    l.setLevel(logging.INFO)
else:
    l.setLevel(logging.WARNING)

def run(cl):
    return subprocess.Popen(cl.split(), 
                         stdout=subprocess.PIPE, 
                         stderr=subprocess.PIPE)

def runReturn(cl):
    return run(cl).communicate()[0].split('\n')

def getSequenceInfo(inputFile):

    if not os.path.exists('seqInfo'):
        os.mkdir('seqInfo')

    seqInfo = {}
    l.debug("get sequence info from %s" % inputFile)


    bamBase = os.path.basename(inputFile).replace('.bam', '')
    seqInfoFile = os.path.join('seqInfo', '%s.seqinfo' % bamBase)

    # see if we can load a pre-prepared seqInfo file - this is a lot faster than
    # going through samtools.
    if os.path.exists(seqInfoFile):
        l.info("Reading cached seqInfo for %s" % bamBase)
        with open(seqInfoFile) as F:
            seqInfo = pickle.load(F)
        return seqInfo

    # nope, use samtools, be patient
    for line in runReturn('samtools view -H %s' % inputFile):
        line = line.strip()
        if not line: continue
        if line[:3] == '@SQ':
            ls = line.split()
            seqId = ls[1][3:]
            seqLen = int(ls[2][3:])
            if not seqInfo.has_key(seqId):
                seqInfo[seqId] = { 'length' : seqLen }
            else:
                if seqInfo[seqId]['length'] != seqLen:
                    raise(Exception("problem with seqlen"))
                seqInfo[seqId]['seenIn'].append(inputFile)

    #write a seqinfo file to disk - for a later call
    with open(seqInfoFile, 'w') as F:
        pickle.dump(seqInfo, F)

    return seqInfo
    
def readBAM(fileName):
    p = run("samtools view -f 2 %s" % fileName)
    while True:
        line1 = p.stdout.readline()
        if not line1: break
        line1 = line1.strip()
        if not line1: continue
        if line1[0] == '@': continue
        line2 = p.stdout.readline()

        l1 = line1.split()
        l2 = line2.split()

        rl = len(l1[9])
        seqid = l1[2]
        start1 = int(l1[3])
        start2 = int(l2[3])

        if start1 > start2:
            start1, start2 = start2, start1
            
        stop1 = start1 + rl
        stop2 = start2 + rl

        yield seqid, start1, stop1, start2, stop2, 

def smoother(a, steps):
    result = np.zeros(len(a) - steps + 1)
    for fr in range(steps):
        to = - (steps - fr - 1)
        if to == 0: to = None
        result += a[fr:to]        
    return result / float(steps)

def doStats(bamname, seqInfo, readPairs):

    if not os.path.exists('stats'):
        os.mkdir('stats')
    stats = {}

    # concatenate all insertsizes of all readpairs of all sequences for
    # this single bamfile into one
    insertSizes = np.concatenate(
        [readPairs[s]['stop2'] - readPairs[s]['start1'] for 
         s in readPairs.keys()])

    l.info("total no readpairs: %d" % len(insertSizes))

    stats['nopairs'] = len(insertSizes)
    stats['median'] = np.median(insertSizes)
    stats['average'] = np.average(insertSizes)


    maxHist = 10 * stats['median']
    
    # generate a histogram
    l.debug('stats %s' % stats)
    l.debug('creating a histogram %s' % str((0, maxHist)))
    hist, edges = np.histogram(insertSizes, bins=500, range=(0, maxHist))
    mids = 0.5 * (edges[1:] + edges[:-1])

    #smooth the histogram (using a moving average)
    shist = smoother(hist, 15)
    smids = smoother(mids, 15)
    
    # find the top of the peak
    top = np.argmax(shist)
    topInsert = smids[top]
    stats['top'] = topInsert
    l.info("insert size tops at %s" % topInsert)
    smoothMax = np.max(shist)

    #find the left & right borders of the peak
    leftB = top-1
    while (shist[leftB] > (0.1 * smoothMax)) and \
            (shist[leftB-1] < shist[leftB]) and \
            leftB > 0: leftB -= 1
    topLeft = smids[leftB]

    rghtB = top+1
    while (shist[rghtB] > (0.1 * smoothMax)) and \
            (shist[rghtB+1] < shist[rghtB]) and \
            rghtB < len(shist): rghtB += 1
    topRight = smids[rghtB]        
    stats['left'] = topLeft
    stats['right'] = topRight

    # plot a figure
    l.info('plotting figure')
    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.title('Insert size distribution for %s' % bamname,
              fontdict={'size' : 10})
    ax.plot(smids, shist, '#8888ff', linewidth=5,
            label = "Smoothed histogram")
    ax.plot(mids, hist, 'red', label='histogram')
    minY, maxY = ax.get_axes().get_ylim()

    ax.vlines(topInsert,minY,maxY, 'g',  linestyles='dotted',
              label='Peak top')
    ax.vlines(topLeft,minY,maxY, 'b', linestyles='dotted', 
              label='left border peak')
    ax.vlines(topRight,minY,maxY, 'r', linestyles='dotted', 
              label='rigth border peak')

    ax.legend(prop={'size' :'x-small'})
    ax.set_xlim(0, min(maxHist, (2 * np.max(insertSizes))))
    plt.savefig(os.path.join('stats', bamname + '.hist.png'))

    #write stats file for this library
    with open(os.path.join('stats', bamname + '.stats'), 'w') as F:
        for k in stats.keys():
            l.debug("stat %s : %s" % (k, stats[k]))
            F.write("%s\t%s\n" % (k, stats[k]))
    
    return stats

def parseBam(seqInfo, inputFileName):

    basename = os.path.basename(inputFileName).replace('.bam', '')

    l.info('processing BAM file: %s' % inputFileName)
    l.info('Basename %s' % basename)
    if not os.path.exists(os.path.join('readpairs', basename)):
        os.makedirs(os.path.join('readpairs', basename))

    readPairs = {}

    minSeqLen = int(float(options.minLen))
    maxInsert  = int(float(options.maxInsert))

    #prep data structure for each sequence
    for seqId in seqInfo.keys():

        if seqInfo[seqId]['length'] < minSeqLen: continue

        readPairs[seqId] = \
            { 'start1' : [],
              'stop1' : [],
              'start2' : [],
              'stop2' : [] }

    l.info("Processing %d sequences < %d nt (from a total of %d)" % (
            len(readPairs), minSeqLen,  len(seqInfo)))

    limit = options.limit
    if not limit:
        limit = int(1e18)

    # start processing bam file
    i = 0; j=0
    for seqId, start1, stop1, start2, stop2 in readBAM(inputFileName):
        j += 1

        if j % 500000 == 0 : l.debug('processed %d readpairs' % j)

        if seqInfo[seqId]['length'] < minSeqLen: continue
        if stop2-start1 > maxInsert: continue

        i += 1
        if i > limit: break
        if i % int(1e7) == 0: l.info("%d reads processed" % i)

        readPairs[seqId]['start1'].append(start1)
        readPairs[seqId]['stop1'].append(stop1)
        readPairs[seqId]['start2'].append(start2)
        readPairs[seqId]['stop2'].append(stop2)

    l.info("discovered %d readpairs (insert < %d nt) out of a total of %d" % ( 
            i, maxInsert, j))
    
    # process the sequences - turn everyting into np.arrays 
    # and write the data to numpy files - for quick reading in 
    # later stage -> for example, when fiddling with the statistics
    for seqId in seqInfo.keys():

        if seqInfo[seqId]['length'] < minSeqLen: continue

        rp = readPairs[seqId]

        rp['start1'] = np.array(rp['start1'])
        rp['stop1'] = np.array(rp['stop1'])
        rp['start2'] = np.array(rp['start2'])
        rp['stop2'] = np.array(rp['stop2'])

        outname = os.path.join('readpairs', basename, seqId + '.readpairs')
        np.savez(outname,
                 start1 = rp['start1'],  
                 stop1 = rp['stop1'],  
                 start2 = rp['start2'],  
                 stop2 = rp['stop2'])
        l.info('wrote %d pairs for %s to disk' % (len(rp['start1']),seqId))
    
    return readPairs

def normalizeSortReadPairs(bamBase, seqInfo, readPairs, stats):
    """
    normalize and sort the readpairs 
    
    normalize -> subtract the peak insert size so that they center
       around zero

    sort into three groups: below zero, around zero and above zero
    
    approach 2 -> sort into multiple groups:

       good -8k
       good -4k
       good -2k
       good -1k
       good
       good + 1k
       good + 2k
       good + 4k
       good + 8k
       good + 16k
       good + 32k
    
    We will do this per bamBase and per sequence
    """    

    if not os.path.exists(os.path.join('coverage', bamBase)):
        os.makedirs(os.path.join('coverage', bamBase))

    minSeqLen = int(float(options.minLen))

    covPlotCount = 0
    for seqId in seqInfo.keys():
        if seqInfo[seqId]['length'] < minSeqLen: continue

        covPlotCount += 1
        if covPlotCount % 5000 == 0:
            l.debug("Calculated %d coverage plots (startin %s now)" % (covPlotCount, seqId))

        l.debug("start calculating coverage plots for %s" % seqId)

        rp = readPairs[seqId]

        seqLen = seqInfo[seqId]['length']

        l.debug("create coverage plot arrays")
        r_ok = np.zeros(seqLen, dtype=np.int)
        r_low = np.zeros(seqLen, dtype=np.int)
        r_high = np.zeros(seqLen, dtype=np.int)

        #approach_2
        bins_low = [1000, 2000, 4000, 8000, 16000, 32000]
        bins_high = [1000, 2000,4000,8000,16000,32000]

        r_low_binned = np.zeros((seqLen, len(bins_low)), dtype=np.int)
        r_high_binned = np.zeros((seqLen, len(bins_high)), dtype=np.int)
        
        peakLow = stats['left']
        peakHigh = stats['right']
                           
        def ff(a,b):
            dist = b-a
            if dist < peakLow:
                tooLow = peakLow - dist
                for binNo in range(len(bins_low)):
                    if tooLow < bins_low[binNo]: break
                #l.info("LW: %s <-%s- (%s-%s) bin %d (%d)" % (dist,  tooLow, peakLow, peakHigh, bins_low[binNo], binNo))
                r_low[a:b] += 1
                r_low_binned[a:b,binNo] += 1
            elif dist > peakHigh:
                tooHigh = dist - peakHigh
                for binNo in range(len(bins_high)):
                    if tooHigh < bins_high[binNo]: break
                #l.info("HG: (%s-%s) ==%s==> %s bin %d (%d)" % (peakLow, peakHigh, tooHigh, dist, bins_high[binNo], binNo))
                r_high[a:b] += 1
                r_high_binned[a:b,binNo] += 1
            else:
                r_ok[a:b] += 1

        FF=np.vectorize(ff)
        l.debug("Start calculating coverage plots for %d readpairs" % len(rp['start1']))
        if len(rp['start1']) > 0:
            FF(rp['start1'], rp['stop2'])
            l.debug("Done calculating coverage plot")
        else:
            l.debug("Not much to do for zero readpairs")

        
        #write the plots to disk      
        l.debug("Start wrinting coverage plot to disk")

        np.savez(os.path.join('coverage', bamBase, seqId + '.coverage'),
                 r_low = r_low, 
                 r_ok = r_ok, 
                 r_high = r_high,
                 r_high_binned = r_high_binned,
                 r_low_binned = r_low_binned,
                 bins_low = bins_low,
                 bins_high = bins_high )
        l.debug("wrote coverage plot to disk")

def readBamDataFromDisk(seqInfo, bamBase):
    readPairs = {}
    minSeqLen = int(float(options.minLen))
    for seqId in seqInfo.keys():
        l.debug("reading bam data for seqId: %s" % seqId)
        if seqInfo[seqId]['length'] < minSeqLen: continue
        
        rp = {}
        datafile = os.path.join('readpairs', bamBase, seqId + '.readpairs.npz')
        data = np.load(datafile)

        #filter out those readpairs that are below a cutoff length
        maxInsert  = int(float(options.maxInsert))
        goodPairs = np.less_equal(data['stop2'] - data['start1'], maxInsert)

        rp['start1'] = np.extract(goodPairs, data['start1'])
        rp['stop1'] = np.extract(goodPairs, data['stop1'])
        rp['start2'] = np.extract(goodPairs, data['start2'])
        rp['stop2'] = np.extract(goodPairs, data['stop2'])
        
        noRecs = rp['start1'].shape[0]
        if noRecs > 0:
            l.info('%s records from %s' % (noRecs, seqId))
        readPairs[seqId] = rp

    return readPairs

def readStatsFromDisk(bamBase):
    stats = {}
    with open(os.path.join('stats', bamBase + '.stats'), 'r') as F:
        for  line in F.readlines():
            k, v = line.strip().split("\t")
            stats[k] = float(v)
    return stats

if __name__ == '__main__':

    if options.onlySeqInfo:
        inFile = inputFiles[0]
        l.info("getting seqInfo from %s" % inFile)
        seqInfo = getSequenceInfo(inFile)
        sys.exit()               

    for inputFile in inputFiles:
        bamBase = os.path.basename(inputFile).replace('.bam', '')
        l.info('processing bamfile %s' % bamBase)

        seqInfo = getSequenceInfo(inputFile)
        l.info('discovered %d sequences' % len(seqInfo))

        # should we pars the BAM files & or read intermediate files
        # from disk?
        if options.skipRead:
            l.info("skiping BAM parsing")
            readPairs = readBamDataFromDisk(seqInfo, bamBase)
        else:
            readPairs = parseBam(seqInfo, inputFile)

        # Should we do the stats? Or read the stats from disk??
        if options.skipStat:
            stats = readStatsFromDisk(bamBase)
            l.info("read stats, peak is at %.0f" % stats['top'])
        else:
            stats = doStats(bamBase, seqInfo, readPairs)

        if not options.skipCoverage:
            #Sort the readpairs
            normalizeSortReadPairs(bamBase, seqInfo, readPairs, stats)


