#!/usr/bin/env python
#
# Copyright 2010 Mark Fiers
# This tool is part of Hagfish & distributed under the GPL
# please see COPYING for details
#

import os
import sys
import pickle

import numpy as np
import matplotlib as mpl
mpl.use('agg')
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab

import logging
import optparse
import subprocess
import cPickle
import gzip

from hagfish_file_util import * 
    
## Arguments: General options 
parser = optparse.OptionParser()
parser.add_option('-v', dest='verbose', action="count", 
                  help='Show debug information')
parser.add_option('-x', dest='limit', type="int",
                  help='Limit the number of pairs to read per input file')
parser.add_option('--onlySeqInfo', dest='onlySeqInfo', action='store_true',
                  help='only process the seqInfo of the first bam input file')

parser.add_option('--low', dest='oklow', type="int", default=0,
                  help='Lower "correct" insert size limit - use this to force ' +
                  'hagfish to a "ok" range')

parser.add_option('--high', dest='okhigh', type="int", default=0,
                  help='Higher "correct" insert size limit - use this to force ' +
                  'hagfish to a "ok" range')

parser.add_option('--skip_read' , dest='skipRead', action="store_true",
                  help='Skip reading the BAM files, read the parsed BAM files ' + 
                  'directly from disk')
parser.add_option('--skip_stats' , dest='skipStat', action="store_true",
                  help='Skip stat creating, read the stat data from disk')

parser.add_option('--skip_plot' , dest='skipPlot', action="store_true",
                  help='Skip generating a plot')

parser.add_option('--skip_coverage' , dest='skipCoverage', action="store_true",
                  help='Skip generating of the coverage plots')

parser.set_defaults(maxInsert='20000')
parser.add_option('--max', dest='maxInsert', 
                   help='max insert size to consider while creating the '+
                  'coverage plots')

parser.set_defaults(minLen='10000')
parser.add_option('--minLen', dest='minLen',
                  help='Minimal sequence length - ignore sequence shorter than ' +
                  'this length')

options, args = parser.parse_args()
inputFiles = args
l = logging.getLogger('hagfish')
handler = logging.StreamHandler()
logmark = chr(27) + '[0;37;44mHAGFISH' + \
          chr(27) + '[0m ' 

formatter = logging.Formatter(
    logmark + '%(levelname)-6s %(message)s')

handler.setFormatter(formatter)
l.addHandler(handler)

if options.verbose >= 2:
    l.setLevel(logging.DEBUG)
elif options.verbose == 1:
    l.setLevel(logging.INFO)
else:
    l.setLevel(logging.WARNING)

def run(cl):
    return subprocess.Popen(cl.split(), 
                         stdout=subprocess.PIPE, 
                         stderr=subprocess.PIPE)

#abc
def runReturn(cl):
    return run(cl).communicate()[0].split('\n')

def getSequenceInfo(inputFile):

    if not os.path.exists('seqInfo'):
        os.mkdir('seqInfo')

    seqInfo = {}
    l.debug("get sequence info from %s" % inputFile)
    

    bamBase = os.path.basename(inputFile).replace('.bam', '')
    seqInfoFile = os.path.join('seqInfo', '%s.seqinfo' % bamBase)

    # see if we can load a pre-prepared seqInfo file - this is a lot faster than
    # going through samtools.
    if os.path.exists(seqInfoFile):
        l.info("Reading cached seqInfo for %s" % bamBase)
        with open(seqInfoFile) as F:
            seqInfo = pickle.load(F)
        return seqInfo

    # nope, use samtools, be patient
    l.debug("start reading sequence info")
    for line in runReturn('samtools view -H %s' % inputFile):
        line = line.strip()
        if not line: continue
        if line[:3] == '@SQ':
            ls = line.split()
            seqId = ls[1][3:]
            seqLen = int(ls[2][3:])
            if not seqInfo.has_key(seqId):
                seqInfo[seqId] = { 'length' : seqLen }
            else:
                if seqInfo[seqId]['length'] != seqLen:
                    raise(Exception("problem with seqlen"))
                #seqInfo[seqId]['seenIn'].append(inputFile)

    #write a seqinfo file to disk - for a later call
    with open(seqInfoFile, 'w') as F:
        pickle.dump(seqInfo, F)

    return seqInfo
    
def readBAM(fileName):
    i = 0
    p = run("samtools view -f 67 %s" % fileName)
    while True:
        line = p.stdout.readline()
        if not line: break
        line = line.strip()
        if not line: continue
        if line[0] == '@': continue
        i += 1
        
        l1 = line.split()

        rl = len(l1[9])
        seqid = l1[2]
        start1 = int(l1[3])
        start2 = int(l1[7])

        if start1 > start2:
            start1, start2 = start2, start1
            
        stop1 = start1 + rl
        stop2 = start2 + rl

        if i < 10:
            l.debug("read %4d : < %6d --(%3d)-- %6d > in %6d out %6d  < %6d --(%3d)-- %6d >" % (
                i, start1, stop1 - start1, stop1,
                start2 - stop1,
                stop2 - start1,
                start2, stop2-start2, stop2))
        yield seqid, start1, stop1, start2, stop2, 

def smoother(a, steps):
    result = np.zeros(len(a) - steps + 1)
    for fr in range(steps):
        to = - (steps - fr - 1)
        if to == 0: to = None
        result += a[fr:to]        
    return result / float(steps)

def doStats(bamname, seqInfo, readPairs):

    if not os.path.exists('stats'):
        os.mkdir('stats')
    stats = {}

    # concatenate all insertsizes of all readpairs of all sequences for
    # this single bamfile into one
    insertSizes = np.concatenate(
        [readPairs[s]['stop2'] - readPairs[s]['start1'] for 
         s in readPairs.keys()])

    l.info("total no readpairs: %d" % len(insertSizes))

    stats['nopairs'] = len(insertSizes)
    stats['median'] = np.median(insertSizes)
    stats['average'] = np.average(insertSizes)


    maxHist = 10 * stats['median']
    
    # generate a histogram
    l.debug('stats %s' % stats)
    l.debug('creating a histogram %s' % str((0, maxHist)))
    hist, edges = np.histogram(insertSizes, bins=500, range=(0, maxHist))
    mids = 0.5 * (edges[1:] + edges[:-1])

    #smooth the histogram (using a moving average)
    shist = smoother(hist, 5)
    smids = smoother(mids, 5)
    
    # find the top of the peak
    top = np.argmax(shist)
    topInsert = smids[top]
    stats['top'] = topInsert
    l.info("insert size tops at %s" % topInsert)
    smoothMax = np.max(shist)

    #find the left & right borders of the peak
    leftB = top-1
    while (shist[leftB] > (0.1 * smoothMax)) and \
            (shist[leftB-1] < shist[leftB]) and \
            leftB > 0: leftB -= 1
    topLeft = smids[leftB]

    rghtB = top+1
    while (shist[rghtB] > (0.1 * smoothMax)) and \
            (shist[rghtB+1] < shist[rghtB]) and \
            rghtB < len(shist): rghtB += 1
    topRight = smids[rghtB]        


    if options.oklow > 0:
        l.info("Forcing min ok insert size to %s" % options.oklow)
        stats['left'] = options.oklow
    else:
        l.info("Estimating min ok insert size as %s" % topLeft)
        stats['left'] = topLeft
        
    if options.okhigh > 0:
        l.info("Forcing max ok insert size to %s" % options.okhigh)
        stats['right'] = options.okhigh
    else:
        l.info("Estimating max ok insert size as %s" % topRight)
        stats['right'] = topRight

    if not options.skipPlot:
        
        # plot a figure - normal histogram
        l.info('plotting normal figure')
        fig = plt.figure()
        ax = fig.add_subplot(111)
        plt.title('Insert size distribution for %s' % bamname,
                  fontdict={'size' : 10})
        ax.plot(mids, hist, 'red', label='histogram')
        ax.plot(smids, shist, '#0000aa', linewidth=2,
                label = "Smoothed histogram")        
        minY, maxY = ax.get_axes().get_ylim()
        if minY < 1:
            minY =1
            ax.get_axes().set_ylim([minY, maxY])
        if minY<= 0: minY = 1
        ax.vlines(topInsert,minY,maxY, 'g',  linestyles='dotted',
                  label='Peak top (%d)' % int(topInsert))
        ax.vlines(topLeft,minY,maxY, 'b', linestyles='dotted', 
                  label='estimated left border peak (%d)' % int(topLeft))
        ax.vlines(topRight,minY,maxY, 'r', linestyles='dotted', 
                  label='estimated right border peak (%d)' % int(topRight))

        if options.oklow > 0:
            ax.vlines(options.oklow,minY,maxY, 'b', linestyles='dashed', 
                      label='forced left border peak (%d)' % options.oklow)
        if options.okhigh > 0:
            ax.vlines(options.okhigh,minY,maxY, 'r', linestyles='dashed', 
                      label='forced right border peak (%d)' % options.okhigh)

        ax.legend(prop={'size' :'x-small'})

        ax.set_xlim(0, min(maxHist, (2 * np.max(insertSizes))))
        plt.savefig(os.path.join('stats', bamname + '.hist.png'))

        # Do a log scaled version of the same figure
        ax.set_yscale('log')
        plt.savefig(os.path.join('stats', bamname + '.log.hist.png'))

        # Do a log scaled version of the same figure
        ax.set_yscale('log')
        ax.set_xscale('log')
        first_non_zero = np.flatnonzero(shist)[0]-1
        #if first_non_zero > 0: first_non_zero = 1
        xMin, xMax = ax.get_axes().get_xlim()
        ax.get_axes().set_xlim([ smids[first_non_zero], xMax])
        #print shist[first_non_zero]
        #print smids[first_non_zero]
        
        plt.savefig(os.path.join('stats', bamname + '.log2.hist.png'))

        # And a thumbnail figure as well
        l.info('plotting thumbnail figure')

        fig = plt.figure()
        ax = plt.axes([0,0,1,1])
        #ax = fig.axes([0,0,1,1], frame_on=False)
        ax.xaxis.set_visible(False)
        ax.yaxis.set_visible(False)
        ax.fill_between(
            mids, hist,
            where=hist>0,
            facecolor='red',
            edgecolor=None)
        minY, maxY = ax.get_axes().get_ylim()
        if options.oklow > 0:
            ax.vlines(topLeft,minY,maxY, 'b', linestyles='dashed')
        else:
            ax.vlines(topLeft,minY,maxY, 'b', linestyles='dotted')

        if options.okhigh > 0:
            ax.vlines(topRight,minY,maxY, 'r', linestyles='dashed')
        else:
            ax.vlines(topRight,minY,maxY, 'r', linestyles='dotted')

        ax.set_xlim(0, min(maxHist, (2 * np.max(insertSizes))))
        plt.savefig(
            os.path.join('stats', bamname + '.thumb.hist.png'),
            dpi=20)

    #write stats file for this library
    with open(os.path.join('stats', bamname + '.stats'), 'w') as F:
        for k in stats.keys():
            l.debug("stat %s : %s" % (k, stats[k]))
            F.write("%s\t%s\n" % (k, stats[k]))
    
    return stats

def parseBam(seqInfo, inputFileName):

    basename = os.path.basename(inputFileName).replace('.bam', '')

    l.info('processing BAM file: %s' % inputFileName)
    l.info('Basename %s' % basename)
    if not os.path.exists(os.path.join('readpairs', basename)):
        os.makedirs(os.path.join('readpairs', basename))

    readPairs = {}

    minSeqLen = int(float(options.minLen))
    maxInsert  = int(float(options.maxInsert))

    #prep data structure for each sequence
    for seqId in seqInfo.keys():

        if seqInfo[seqId]['length'] < minSeqLen: continue

        readPairs[seqId] = \
            { 'start1' : [],
              'stop1' : [],
              'start2' : [],
              'stop2' : [] }

    l.info("Processing %d sequences < %d nt (from a total of %d)" % (
            len(readPairs), minSeqLen,  len(seqInfo)))

    limit = options.limit
    if not limit:
        limit = int(1e18)

    # start processing bam file
    i = 0; j=0
    for seqId, start1, stop1, start2, stop2 in readBAM(inputFileName):
        j += 1

        if j % 500000 == 0 : l.debug('processed %d readpairs' % j)

        if seqInfo[seqId]['length'] < minSeqLen: continue
        if stop2-start1 > maxInsert: continue

        i += 1
        if i > limit: break
        if i % int(1e7) == 0: l.info("%d reads processed" % i)

        readPairs[seqId]['start1'].append(start1)
        readPairs[seqId]['stop1'].append(stop1)
        readPairs[seqId]['start2'].append(start2)
        readPairs[seqId]['stop2'].append(stop2)

    l.info("discovered %d readpairs (insert < %d nt) out of a total of %d" % ( 
            i, maxInsert, j))
    
    # process the sequences - turn everyting into np.arrays 
    # and write the data to numpy files - for quick reading in 
    # later stage -> for example, when fiddling with the statistics
    no_with_zero_pairs = 0
    for seqId in seqInfo.keys():

        
        if seqInfo[seqId]['length'] < minSeqLen: continue

        rp = readPairs[seqId]

        rp['start1'] = np.array(rp['start1'])
        rp['stop1'] = np.array(rp['stop1'])
        rp['start2'] = np.array(rp['start2'])
        rp['stop2'] = np.array(rp['stop2'])

        if len(rp['start1']) > 0:
            outname = os.path.join('readpairs', basename, seqId + '.readpairs')
            np_savez(outname,
                     start1 = rp['start1'],  
                     stop1 = rp['stop1'],  
                     start2 = rp['start2'],  
                     stop2 = rp['stop2'])
            if len(rp['start1']) > 0:
                l.info('wrote %d pairs for %s to disk' % (len(rp['start1']),seqId))
        else:
            no_with_zero_pairs += 1
    l.info('wroted data for %d sequences with zero pairs' % (no_with_zero_pairs))
    return readPairs

def normalizeSortReadPairs(bamBase, seqInfo, readPairs, stats):
    """
    normalize and sort the readpairs 
    
    normalize -> subtract the peak insert size so that they center
       around zero

    sort into three groups: below zero, around zero and above zero
    
    approach 2 -> sort into multiple groups:

       good -8k
       good -4k
       good -2k
       good -1k
       good
       good + 1k
       good + 2k
       good + 4k
       good + 8k
       good + 16k
       good + 32k
    
    We will do this per bamBase and per sequence
    """    

    if not os.path.exists(os.path.join('coverage', bamBase)):
        os.makedirs(os.path.join('coverage', bamBase))

    minSeqLen = int(float(options.minLen))

    covPlotCount = 0
    no_seqs_with_zero_readpairs = 0
    for seqId in seqInfo.keys():
        if seqInfo[seqId]['length'] < minSeqLen: continue

        covPlotCount += 1
        if covPlotCount % 5000 == 0:
            l.debug("Calculated %d coverage plots (starting %s now)" % (covPlotCount, seqId))


        if not readPairs.has_key(seqId):
            #assume that this data has not been loaded
            no_seqs_with_zero_readpairs += 1
            continue
        
        rp = readPairs[seqId]
        
        if len(rp['start1']) == 0:
            no_seqs_with_zero_readpairs += 1
            continue
        
        l.debug("start calculating coverage plots for %s (%d)" % (seqId, len(rp)))

        seqLen = seqInfo[seqId]['length']

        l.debug("create coverage plot arrays")
        r_ok = np.zeros(seqLen, dtype=np.int)
        r_low = np.zeros(seqLen, dtype=np.int)
        r_high = np.zeros(seqLen, dtype=np.int)

        r_ok_ends = np.zeros(seqLen, dtype=np.int)
        r_low_ends = np.zeros(seqLen, dtype=np.int)
        r_high_ends = np.zeros(seqLen, dtype=np.int)

        peakLow = stats['left']
        peakHigh = stats['right']
                           
        def ff(a1, a2, b1, b2):
            a = a1
            b = b2
            dist = b2 - a1
            if dist < peakLow:
                r_low[a1:b2] += 1
                r_low_ends[a1:a2] += 1
                r_low_ends[b1:b2] += 1
            elif dist > peakHigh:
                r_high[a1:b2] += 1
                r_high_ends[a1:a2] += 1
                r_high_ends[b1:b2] += 1
            else:
                r_ok[a:b] += 1
                r_ok_ends[a1:a2] += 1
                r_ok_ends[b1:b2] += 1

        FF=np.vectorize(ff)
        l.debug("Start calculating coverage plots for %d readpairs" % len(rp['start1']))
        if len(rp['start1']) > 0:
            FF(rp['start1'], rp['stop1'], rp['start2'], rp['stop2'])
            l.debug("Done calculating coverage plot")
        else:
            l.debug("Not much to do for zero readpairs")

        
        #write the plots to disk      
        l.debug("Start wrinting coverage plot to disk")

        np_savez(os.path.join('coverage', bamBase, seqId + '.coverage'),
                 r_low = r_low, 
                 r_ok = r_ok, 
                 r_high = r_high,
                 r_low_ends = r_low_ends, 
                 r_ok_ends = r_ok_ends, 
                 r_high_ends = r_high_ends)

        l.debug("wrote coverage plot to disk")
    l.debug("skipped %d sequences with zero readpairs" % no_seqs_with_zero_readpairs)

def readBamDataFromDisk(seqInfo, bamBase):
    readPairs = {}
    minSeqLen = int(float(options.minLen))
    no_skipped = 0
    for seqId in seqInfo.keys():
        if seqInfo[seqId]['length'] < minSeqLen: continue
        
        rp = {}
        maxInsert  = int(float(options.maxInsert))
        file_base = os.path.join('readpairs', bamBase, seqId + '.readpairs')
        try:
            start1 = np_load(file_base, 'start1')
            stop1 = np_load(file_base, 'stop1')
            start2 = np_load(file_base, 'start2')
            stop2 = np_load(file_base, 'stop2')
        except IOError:
            #assume that this file has not been written - i.e.
            #no readpairs to speak of - so, ignore
            no_skipped += 1
            continue

        l.debug("reading bam data for seqId: %s" % seqId)

        #filter out those readpairs that are below a cutoff length
        goodPairs = np.less_equal(stop2 - start1, maxInsert)

        rp['start1'] = np.extract(goodPairs, start1)
        rp['stop1'] = np.extract(goodPairs, stop1)
        rp['start2'] = np.extract(goodPairs, start2)
        rp['stop2'] = np.extract(goodPairs, stop2)
        
        noRecs = rp['start1'].shape[0]
        if noRecs > 0:
            l.info('%s records from %s' % (noRecs, seqId))
        readPairs[seqId] = rp
    l.debug("skipped reading bam for %d sequences" % no_skipped)
    return readPairs

def readStatsFromDisk(bamBase):
    stats = {}
    with open(os.path.join('stats', bamBase + '.stats'), 'r') as F:
        for  line in F.readlines():
            k, v = line.strip().split("\t")
            stats[k] = float(v)
    return stats

if __name__ == '__main__':

    if options.onlySeqInfo:
        inFile = inputFiles[0]
        l.info("getting seqInfo from %s" % inFile)
        seqInfo = getSequenceInfo(inFile)
        sys.exit()               

    for inputFile in inputFiles:
        bamBase = os.path.basename(inputFile).replace('.bam', '')
        l.info('processing bamfile %s' % bamBase)

        seqInfo = getSequenceInfo(inputFile)
        l.info('discovered %d sequences' % len(seqInfo))

        # should we pars the BAM files & or read intermediate files
        # from disk?
        if options.skipRead:
            l.info("skiping BAM parsing")
            readPairs = readBamDataFromDisk(seqInfo, bamBase)
        else:
            readPairs = parseBam(seqInfo, inputFile)

        # Should we do the stats? Or read the stats from disk??
        if options.skipStat:
            stats = readStatsFromDisk(bamBase)
            l.info("read stats, peak is at %.0f" % stats['top'])
        else:
            stats = doStats(bamBase, seqInfo, readPairs)

        if not options.skipCoverage:
            #Sort the readpairs
            normalizeSortReadPairs(bamBase, seqInfo, readPairs, stats)

